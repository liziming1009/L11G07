---
title: "Wine Quality"
subtitle: "Group L11G07"
author: "Yanxi Chen, Sabrina Chiu, Jikai Hu, Yiyang Li and Harper Pan"
format: 
  revealjs:
    auto-slide: 20000
    smaller: true
    theme: simple
    slide-number: true
    transition: fade
    embed-resources: true
execute:
  echo: false
  warning: false
  message: false
---

```{r echo=FALSE}
library("kableExtra")
library("tidyverse")
library("janitor")
library("knitr")
library("dplyr")
library("ggplot2")
library("ggpubr")
library("qtlcharts")
library("car")
library("caret")
library("corrplot")
library("rpart")
```

## Data Background

-   **Dataset:** Wine Quality dataset (UCI)
-   **Rows**: \~6,500 (Red & White Wine)
-   **Features:** acidity, residual sugar, sulphates, alcohol, density, etc. (Total of 13 variables, including the target and type).
-   **Target**: quality (0–10 scale)
-   **Data source:** UCI Machine Learning Repository

```{r}

red  <- read.csv("winequality-red.csv", sep = ";")
white<- read.csv("winequality-white.csv", sep = ";")
red$type <- "red"
white$type <- "white"
wine <- bind_rows(red, white) |> clean_names()
head(wine,n=3)
```

------------------------------------------------------------------------

## Motivation

-   **Dataset:** Wine quality with multiple chemical features
-   **Observation:** Quality affected by multiple factors, not just one
-   **Real-world challenge:** Limited budget → cannot improve all features
-   **Research focus:** → Find which features give the highest return on quality improvement → Support better resource allocation and decision-making

------------------------------------------------------------------------

## Research Question

**Which features contribute most to predicting wine quality?**

-   Real-world Impact / Practical Significance
    -   Provide data-driven insight into the key drivers of wine quality
    -   Help winemakers optimize production decisions under limited budgets
    -   Identify the most cost-effective variables to improve product quality
    -   Bridge the gap between chemical analysis and consumer perception

**Which features contribute most to predicting wine quality?**

-   Real-world relevance:
    -   Guide winemakers’ investment priorities
    -   Support data-driven quality management

## Data cleaning：Combined red and white wine datasets (≈ 6,500 samples)

-   12 chemical variables + type label
-   **Target variable:** Quality (score from 3 to 8)
-   No missing values; categorical → numeric conversion

```{r}
# Clean the data
wine_clean <- wine %>% filter(complete.cases(.))
```

## Outlier Removal

-   Identify and remove outliers in the data
-   Based on 1.5 \* IQR rule
-   Function to remove outliers in numeric columns

```{r}
# Remove outliers
remove_outliers <- function(df) {
  df %>%
    filter(if_all(where(is.numeric), ~ {
      Q1 <- quantile(., 0.25, na.rm = TRUE)
      Q3 <- quantile(., 0.75, na.rm = TRUE)
      IQR <- Q3 - Q1
      . >= (Q1 - 1.5 * IQR) & . <= (Q3 + 1.5 * IQR)
    }))
}
wine_no_outliers <- remove_outliers(wine)
# check missing data
colSums(is.na(wine))
```

------------------------------------------------------------------------

## EDA – Quality Distribution & Comparison

-   **Target Distribution:** Analyze the overall spread and skewness of wine quality scores.
-   **Quality by Type:** Compare the central tendency and variation of quality between Red and White wines.

```{r}
#| label: quality-eda-combined
#| fig-align: center
#| echo: false
#| message: false
#| warning: false

# 1. Distribution of Quality
p1 <- ggplot(wine, aes(x = quality)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(title = "Overall Quality Distribution", x="Quality", y="Count") +
  theme_minimal()

# 2. Quality by Wine Type
p2 <- ggplot(wine, aes(x = type, y = quality, fill = type)) +
  geom_boxplot() +
  labs(title = "Quality by Wine Type", x=NULL) +
  theme_minimal() +
  theme(legend.position = "none")

# Combine the two plots side-by-side
ggpubr::ggarrange(p1, p2, ncol = 2)
```

------------------------------------------------------------------------

## EDA – Correlation Matrix

-   Visually explore linear relationships among all numerical variables.
-   Identify strong correlations with $\text{quality}$, and spot inter-correlations leading to multicollinearity.

```{r}
#| label: corr-matrix-plot
#| fig-align: center   
#| fig-height: 8
#| fig-width: 8
wine_corr <- cor(wine %>% select(where(is.numeric)))

corrplot(
  wine_corr,
  method = "color",          
  type = "upper",           
  diag = FALSE,          
  order = "hclust",      
  addCoef.col = "black",   
  tl.col = "black",       
  tl.srt = 45,             
  mar = c(0, 0, 1, 0)       
)
```

**Key Predictors:** alcohol, density, chlorides

------------------------------------------------------------------------

## Distribution Visualization and Strategy

-   **Alcohol:** Right-skewed distribution, peaking between $9-11\%$.
-   **Chlorides:** Highly right-skewed; log transformation is suggested for modeling.
-   **Density:** Highly concentrated distribution, making it a sensitive predictor.

```{r}
#| label: dist-viz
#| echo: false
#| fig-align: center

# Distribution visualization
wine %>%
 select(alcohol, density, chlorides) %>%
 pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
 ggplot(aes(x = value)) +
 geom_histogram(bins = 30, fill = "skyblue", color = "white") +
 facet_wrap(~variable, scales = "free") +
 theme_minimal()
```

------------------------------------------------------------------------

## Model 1 – Initial Linear Regression

-   Used all 12 predictors + type

-   Weak performance (Adj R² ≈ 0.34)

-   Signs of multicollinearity & redundant variables

-   Residuals not well-behaved

-   → Stepwise (AIC) selection to improve efficiency and interpretability

```{r, echo=FALSE, message=FALSE, warning=FALSE}
dat <- wine |> dplyr::mutate(type = factor(type))
m0 <- lm(quality ~ ., data = dat)
m1 <- step(m0, trace = 0)
vif_values <- car::vif(m1)
vif_df <- tibble(Variable = names(vif_values), VIF = as.numeric(vif_values))

# ✅ 只画 VIF 图
ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF, fill = VIF > 5)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_hline(yintercept = 5, linetype = 2, color = "red") +
  scale_fill_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "VIF after Stepwise Selection (M1)",
    subtitle = "Variables with VIF > 5 indicate strong multicollinearity",
    x = NULL, y = "Variance Inflation Factor (VIF)"
  ) +
  theme_minimal(base_size = 14)
```

------------------------------------------------------------------------

## Data Engineering

-   Stepwise model still included too many correlated variables
-   Performed multicollinearity check (VIF) → identified strong correlations
-   Designed new composite variables to reduce redundancy

```{r}

vif_values <- vif(m1)
print(vif_values)

```

------------------------------------------------------------------------

## Constructed New Composite Variables to Reduce Multicollinearity

-   **sulfur_ratio** = free / total sulfur dioxide\
-   **sugar_density_diff** = scaled residual sugar − scaled density\
-   **acid_index** = mean of standardized acidity-related variables\
-   Capture balance, purity, and acidity intensity

```{r, echo=FALSE, message=FALSE, warning=FALSE}
wine_num <- wine %>%
  transmute(
    alcohol, chlorides, sulphates, density, type, quality,
    free_sulfur_dioxide, total_sulfur_dioxide, residual_sugar,
    fixed_acidity, volatile_acidity,
    sulfur_ratio       = free_sulfur_dioxide / total_sulfur_dioxide,
    sugar_density_diff = residual_sugar - density,
    acid_index         = fixed_acidity / volatile_acidity,
    log_quality        = log(quality),
    type               = factor(type)
  )

# Define the M2 model before checking VIF
m2 <- lm(
  quality ~ sulfur_ratio + sugar_density_diff + acid_index +
    alcohol + chlorides + sulphates + density + type,
  data = wine_num
)
```

------------------------------------------------------------------------

## Multicollinearity Check

-   Checked multicollinearity among predictors in **M2**.
-   **VIF \< 5** → acceptable; no severe collinearity detected.
-   Ensures variables contribute independently to explaining quality.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
vif_df <- data.frame(
  Variable = names(vif(m2)),
  VIF = vif(m2)
)

library(ggplot2)
ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF, fill = VIF > 5)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_hline(yintercept = 5, linetype = 2, color = "red") +
  scale_fill_manual(values = c("steelblue", "tomato")) +
  labs(title = "VIF values for predictors in M2",
       x = NULL, y = "Variance Inflation Factor (VIF)") +
  theme_minimal(base_size = 14)
```

------------------------------------------------------------------------

## Final Dataset Overview

-   Used data after **Stepwise (AIC)** selection and **feature engineering**
-   Final dataset includes:
    -   **\~6,500 rows**, **14 variables**

    -   3 new engineered ratios:

        -   sulfur_ratio = free / total sulfur dioxide
        -   sugar_density_diff = residual_sugar - density
        -   acid_index = fixed_acidity / volatile_acidity

```{r, echo=FALSE, message=FALSE, warning=FALSE}
        wine_final <- wine_num %>%
          select(quality, sulfur_ratio, sugar_density_diff, acid_index,
                 alcohol, chlorides, sulphates, density, type)

        knitr::kable(head(wine_final, 6), caption = "Final Engineered Dataset (Sample Rows)") %>%
          kable_styling(full_width = FALSE, font_size = 14)
```

------------------------------------------------------------------------

## Modeling on Engineered Dataset (M2–M5)

-   Trained four functional forms on the cleaned and engineered dataset:
    -   **M2:** Linear–Linear
    -   **M3:** Log–Linear (log y)
    -   **M4:** Linear–Log (log X)
    -   **M5:** Log–Log (both)
-   Used **10-fold cross-validation** to ensure robustness.
-   Compared performance across RMSE, MAE, and R².
    -   For fair comparison, all models using log(quality) as the response were **back-transformed to the original quality scale** before computing metrics.\

```{r, echo=FALSE, message=FALSE, warning=FALSE}
stopifnot("type" %in% names(wine_num))
wine_num$type <- factor(wine_num$type, levels = c("white", "red"))

drop_cols <- intersect(names(wine_num),
                       c("typered", "typewhite", "type_red", "type_white"))
if (length(drop_cols) > 0) wine_num[drop_cols] <- NULL

options(contrasts = c("contr.treatment", "contr.poly"))

if (!"log_quality" %in% names(wine_num)) {
  wine_num$log_quality <- log(wine_num$quality)
}

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(caret)
set.seed(1)

ctrl <- trainControl(method = "cv", number = 10, savePredictions = "final")

cv_m2 <- train(
  quality ~ sulfur_ratio + sugar_density_diff + acid_index +
    alcohol + chlorides + sulphates + density + type,
  data = wine_num, method = "lm", trControl = ctrl
)

cv_m3 <- train(  # log-linear (log y)
  log_quality ~ sulfur_ratio + sugar_density_diff + acid_index +
    alcohol + chlorides + sulphates + density + type,
  data = wine_num, method = "lm", trControl = ctrl
)

cv_m4 <- train(  # linear-log (log X only)
  quality ~ sulfur_ratio + sugar_density_diff + acid_index +
    alcohol + log(chlorides) + sulphates + density + type,
  data = wine_num, method = "lm", trControl = ctrl
)

cv_m5 <- train(  # log-log (log y & X)
  log_quality ~ sulfur_ratio + sugar_density_diff + acid_index +
    alcohol + log(chlorides) + sulphates + density + type,
  data = wine_num, method = "lm", trControl = ctrl
)
```

------------------------------------------------------------------------

## Comparison of Four Models

-   Tested four forms: linear–linear, log–linear, linear–log, log–log.\
-   Used 10-fold cross-validation to compare RMSE, MAE, and R².\
-   For fair comparison, all models using log(quality) as the response were **back-transformed to the original quality scale** before computing metrics.\
-   Results show performance differs notably across transforms.\

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tibble)
library(kableExtra)


cv_metrics <- function(fit, label, log_y = FALSE, data = NULL, y_name = "quality") {
  if (!is.null(fit$pred)) {
    preds <- fit$pred %>%
      transmute(
        Resample,
        
        y    = if (log_y) exp(obs)  else obs,
        yhat = if (log_y) exp(pred) else pred
      )

    by_fold <- preds %>%
      group_by(Resample) %>%
      summarise(
        RMSE = sqrt(mean((y - yhat)^2)),
        MAE  = mean(abs(y - yhat)),
        R2   = 1 - sum((y - yhat)^2) / sum((y - mean(y))^2),
        .groups = "drop"
      )

    summarise(by_fold, RMSE = mean(RMSE), MAE = mean(MAE), Rsq = mean(R2)) %>%
      mutate(Model = label, .before = 1)

  } else {
    
    stopifnot(!is.null(data))
    y    <- data[[y_name]]
    yhat <- predict(fit$finalModel, newdata = data)
    if (log_y) {
      
      yhat <- exp(yhat)
    }
    tibble(
      Model = label,
      RMSE  = sqrt(mean((y - yhat)^2)),
      MAE   = mean(abs(y - yhat)),
      Rsq   = 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)
    )
  }
}


rows <- list(
  cv_metrics(cv_m2, "linear-linear (selected)", FALSE, data = wine_num),
  cv_metrics(cv_m4, "linear-log (log X)",       FALSE, data = wine_num),
  cv_metrics(cv_m3, "log-linear (log y)",        TRUE, data = wine_num),
  cv_metrics(cv_m5, "log-log (log y & X)",       TRUE, data = wine_num)
)

cv_compare <- bind_rows(rows) %>% arrange(RMSE)

knitr::kable(cv_compare,
             caption = "Model Comparison: Cross-Validation Results (original scale)",
             digits = 3, align = c("l","c","c","c")) %>%
  kable_styling(font_size = 18, full_width = FALSE,
                bootstrap_options = c("condensed","hover","striped")) %>%
  column_spec(1, bold = TRUE, width = "15em") %>%
  column_spec(2:4, width = "9em") %>%
  row_spec(0, bold = TRUE)

```

------------------------------------------------------------------------

## Model Selection Result (M4)

-   Compared all via 10-fold CV (RMSE, MAE, R²).

-   **M4 (linear–log)** performs best overall, with the lowest RMSE(0.735) and the highest R\^2(0.291), indicating it captures nonlinearity in predictors without distorting the response scale.

-   Balances accuracy, stability, and interpretability.

-   Selected as **final predictive model**.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
sel_name <- cv_compare$Model[which.min(cv_compare$RMSE)]
cv_compare %>%
  mutate(Selected = ifelse(Model == sel_name, "★", "")) %>%
  select(Model, RMSE, Rsq, MAE, Selected) %>%
  knitr::kable(caption = "Model Selection Summary (Chosen on original scale)",
               digits = 3, align = c("l","c","c","c","c")) %>%
  kable_styling(font_size = 16, full_width = FALSE,
                bootstrap_options = c("striped","hover")) %>%
  column_spec(1, bold = TRUE, width = "15em") %>%
  column_spec(2:5, width = "8em")

```

------------------------------------------------------------------------

## M4 Residual Diagnostics

-   Residuals vs Fitted: no strong pattern → linearity OK.

-   Q–Q plot aligns with diagonal → normality acceptable.

-   Minor tail deviation only; assumptions broadly satisfied.

-   Confirms reliability of inferences.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
m4 <- lm(
  quality ~ sulfur_ratio + sugar_density_diff + acid_index +
    alcohol + log(chlorides) + sulphates + density + type,
  data = wine_num
)


library(ggfortify)
autoplot(m4, which = 1:2) + theme_bw()
```

## **Discussion of Results (M4: linear–log)**

-   **Key coefficients (direction):** Quality ↑ with sulphates, sulfur ratio (free/total SO₂), alcohol, sugar–density balance, acid index; Quality ↓ with density and log(chlorides); red vs white shows a baseline shift.

-   **Most influential signals:** Strong negative effect of density; strong positive effects of sulphates and sulfur ratio; alcohol also a meaningful positive driver.

-   **Why R² is modest & models are close:** Quality is **discrete** and narrowly distributed (mostly 5–6) → low total variance → R² clusters around \~0.28–0.29; RMSE differences remain small.

------------------------------------------------------------------------

## Limitations & Risks

**Data:** 'quality' variable is discrete and concentrated; extreme *chlorides* required careful handling; ‘type’ differences may limit external validity.

**Method:** linear additivity and independence assumed; residual collinearity may remain. Explore controlled nonlinearity or tree ensembles with careful regularisation.

**Avenues for future work:** Use **ordinal/ordered** models tailored to discrete scores; Add richer features; Explore controlled nonlinearity or tree ensembles with careful regularisation.

**Business:** correlational model; deployment depends on measurement quality and variable availability.
